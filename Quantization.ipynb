{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Quantization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsanabbas123/dynamic-quantization-LSTM/blob/main/Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNDCER33f_g5"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CORCjq3Af_g6"
      },
      "source": [
        "# imports\n",
        "import os\n",
        "from io import open\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.quantization"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVX5z34AYr7A"
      },
      "source": [
        "### Preprocess "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Fqc-CQf_g6"
      },
      "source": [
        "\"\"\"\n",
        "We define a dictionary and corpus class to hold our WikiText-2 dataset\n",
        "\"\"\"\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsGZ_78vXZR-"
      },
      "source": [
        "# tokenise and preprocess the dataset\n",
        "corpus = Corpus('wikitext-2')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxaX0PTwq4IF"
      },
      "source": [
        "# setting variable values\n",
        "\n",
        "bptt = 35 # sequence length\n",
        "log_interval = 200 # training output logging frequency\n",
        "clip = .25 # gradient clipping\n",
        "lr = 20 # learning rate for optimizer\n",
        "\n",
        "best_val_loss = None # best model amongst epochs\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "dropout = 0.2\n",
        "save = 'model.pt' # path to save our model\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "eval_batch_size = 10\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G-bBDIxf_g6"
      },
      "source": [
        "# divide the dataset into batches and trim off excess tokens\n",
        "def batchify(data, bs):\n",
        "    nbatch = data.size(0) // bs\n",
        "    data = data.narrow(0, 0, nbatch * bs)\n",
        "    data = data.view(bs, -1).t().contiguous()\n",
        "    return data \n",
        "\n",
        "\"\"\"\n",
        "sending the train and val data to GPU but not test data as quantization inference works only on CPU\n",
        "and we need test data for comparing the loss of normal model vs quantized model at the end\n",
        "\"\"\"\n",
        "train_data = batchify(corpus.train, batch_size).to(device)\n",
        "val_data = batchify(corpus.valid, eval_batch_size).to(device)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc4gflK8YvBE"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feY3myYLf_g6"
      },
      "source": [
        "\"\"\"\n",
        "This is a basic LSTM based model with an encoder and decoder module\n",
        "\"\"\"\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ZVoVGdf_g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5dd084-49a8-4213-c4bc-5def96ff5189"
      },
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = LSTMModel(\n",
        "    ntoken = ntokens,\n",
        "    ninp = 512,\n",
        "    nhid = 256,\n",
        "    nlayers = 5,\n",
        ")\n",
        "\n",
        "# sending the model to GPU for training\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(33278, 512)\n",
            "  (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)\n",
            "  (decoder): Linear(in_features=256, out_features=33278, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlENcqWyY3xk"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUb5SweGSj4Y"
      },
      "source": [
        "# Evaluation functions\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "# taken from the pytorch documentation\n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "      return h.detach()\n",
        "  else:\n",
        "      return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def evaluate(model_, data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model_.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model_.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model_(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNKT3Xr6o9h1"
      },
      "source": [
        "def train(model, corpus, train_data, bptt, criterion, clip, lr, log_interval, batch_size, epoch):\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        \n",
        "        loss = criterion(output_flat, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # log the output\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAeU53-UY6Wk"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z_RYkhVshNZ"
      },
      "source": [
        "def train_epochs(model, corpus, train_data, val_data, bptt, criterion, clip, lr, log_interval, batch_size, epochs, save):\n",
        "    # Loop over epochs.\n",
        "    best_val_loss = None\n",
        "    try:\n",
        "        for epoch in range(1, epochs+1):\n",
        "            epoch_start_time = time.time()\n",
        "            train(model, corpus, train_data, bptt, criterion, clip, lr, log_interval, batch_size, epoch)\n",
        "            val_loss = evaluate(model, val_data)\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                    'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                            val_loss, math.exp(val_loss)))\n",
        "            print('-' * 89)\n",
        "            # Save the model if the validation loss is the best we've seen so far.\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                # with open(save, 'wb') as f:\n",
        "                #     torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model\n",
        "            else:\n",
        "                # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "                lr /= 4.0\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "    return best_model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp-9uN1IL9AM",
        "outputId": "dfef7d60-df4d-4f6e-efd7-d73a6a8d125e"
      },
      "source": [
        "model = train_epochs(model, corpus, train_data, val_data, bptt, criterion, clip, lr, log_interval, batch_size, epochs, save)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 1864 batches | lr 20.00 | ms/batch 41.76 | loss  7.74 | ppl  2301.78\n",
            "| epoch   1 |   400/ 1864 batches | lr 20.00 | ms/batch 40.58 | loss  7.33 | ppl  1523.39\n",
            "| epoch   1 |   600/ 1864 batches | lr 20.00 | ms/batch 40.56 | loss  7.22 | ppl  1361.55\n",
            "| epoch   1 |   800/ 1864 batches | lr 20.00 | ms/batch 40.57 | loss  7.19 | ppl  1332.12\n",
            "| epoch   1 |  1000/ 1864 batches | lr 20.00 | ms/batch 40.58 | loss  7.18 | ppl  1317.58\n",
            "| epoch   1 |  1200/ 1864 batches | lr 20.00 | ms/batch 40.57 | loss  7.15 | ppl  1276.18\n",
            "| epoch   1 |  1400/ 1864 batches | lr 20.00 | ms/batch 40.53 | loss  7.17 | ppl  1295.56\n",
            "| epoch   1 |  1600/ 1864 batches | lr 20.00 | ms/batch 40.52 | loss  7.14 | ppl  1259.36\n",
            "| epoch   1 |  1800/ 1864 batches | lr 20.00 | ms/batch 40.50 | loss  7.14 | ppl  1262.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 79.58s | valid loss  6.92 | valid ppl  1012.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1864 batches | lr 20.00 | ms/batch 40.83 | loss  7.13 | ppl  1250.99\n",
            "| epoch   2 |   400/ 1864 batches | lr 20.00 | ms/batch 40.49 | loss  7.11 | ppl  1229.79\n",
            "| epoch   2 |   600/ 1864 batches | lr 20.00 | ms/batch 40.58 | loss  7.08 | ppl  1183.00\n",
            "| epoch   2 |   800/ 1864 batches | lr 20.00 | ms/batch 40.79 | loss  7.10 | ppl  1216.29\n",
            "| epoch   2 |  1000/ 1864 batches | lr 20.00 | ms/batch 41.00 | loss  7.10 | ppl  1216.02\n",
            "| epoch   2 |  1200/ 1864 batches | lr 20.00 | ms/batch 41.22 | loss  7.09 | ppl  1198.77\n",
            "| epoch   2 |  1400/ 1864 batches | lr 20.00 | ms/batch 41.55 | loss  7.11 | ppl  1225.08\n",
            "| epoch   2 |  1600/ 1864 batches | lr 20.00 | ms/batch 41.46 | loss  7.09 | ppl  1203.43\n",
            "| epoch   2 |  1800/ 1864 batches | lr 20.00 | ms/batch 41.52 | loss  7.10 | ppl  1215.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 80.27s | valid loss  6.91 | valid ppl   999.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1864 batches | lr 20.00 | ms/batch 42.19 | loss  7.11 | ppl  1220.17\n",
            "| epoch   3 |   400/ 1864 batches | lr 20.00 | ms/batch 42.22 | loss  7.09 | ppl  1203.46\n",
            "| epoch   3 |   600/ 1864 batches | lr 20.00 | ms/batch 42.41 | loss  6.98 | ppl  1074.89\n",
            "| epoch   3 |   800/ 1864 batches | lr 20.00 | ms/batch 42.49 | loss  6.84 | ppl   936.03\n",
            "| epoch   3 |  1000/ 1864 batches | lr 20.00 | ms/batch 42.67 | loss  6.69 | ppl   806.80\n",
            "| epoch   3 |  1200/ 1864 batches | lr 20.00 | ms/batch 42.66 | loss  6.57 | ppl   710.29\n",
            "| epoch   3 |  1400/ 1864 batches | lr 20.00 | ms/batch 42.67 | loss  6.50 | ppl   665.90\n",
            "| epoch   3 |  1600/ 1864 batches | lr 20.00 | ms/batch 42.72 | loss  6.44 | ppl   627.29\n",
            "| epoch   3 |  1800/ 1864 batches | lr 20.00 | ms/batch 42.75 | loss  6.38 | ppl   589.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 83.02s | valid loss  6.11 | valid ppl   451.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1864 batches | lr 20.00 | ms/batch 43.28 | loss  6.32 | ppl   552.91\n",
            "| epoch   4 |   400/ 1864 batches | lr 20.00 | ms/batch 43.23 | loss  6.25 | ppl   517.03\n",
            "| epoch   4 |   600/ 1864 batches | lr 20.00 | ms/batch 43.30 | loss  6.16 | ppl   473.06\n",
            "| epoch   4 |   800/ 1864 batches | lr 20.00 | ms/batch 43.30 | loss  6.15 | ppl   467.58\n",
            "| epoch   4 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.24 | loss  6.09 | ppl   443.31\n",
            "| epoch   4 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.11 | loss  6.06 | ppl   429.19\n",
            "| epoch   4 |  1400/ 1864 batches | lr 20.00 | ms/batch 42.91 | loss  5.98 | ppl   396.25\n",
            "| epoch   4 |  1600/ 1864 batches | lr 20.00 | ms/batch 42.89 | loss  5.93 | ppl   377.94\n",
            "| epoch   4 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.02 | loss  5.92 | ppl   372.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 84.15s | valid loss  5.71 | valid ppl   302.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1864 batches | lr 20.00 | ms/batch 43.38 | loss  5.89 | ppl   362.31\n",
            "| epoch   5 |   400/ 1864 batches | lr 20.00 | ms/batch 43.15 | loss  5.85 | ppl   346.61\n",
            "| epoch   5 |   600/ 1864 batches | lr 20.00 | ms/batch 43.13 | loss  5.78 | ppl   323.10\n",
            "| epoch   5 |   800/ 1864 batches | lr 20.00 | ms/batch 43.23 | loss  5.80 | ppl   330.24\n",
            "| epoch   5 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.36 | loss  5.77 | ppl   319.45\n",
            "| epoch   5 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.28 | loss  5.76 | ppl   316.82\n",
            "| epoch   5 |  1400/ 1864 batches | lr 20.00 | ms/batch 43.43 | loss  5.69 | ppl   295.26\n",
            "| epoch   5 |  1600/ 1864 batches | lr 20.00 | ms/batch 43.43 | loss  5.66 | ppl   286.23\n",
            "| epoch   5 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.26 | loss  5.66 | ppl   288.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 84.45s | valid loss  5.52 | valid ppl   248.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1864 batches | lr 20.00 | ms/batch 43.32 | loss  5.65 | ppl   283.94\n",
            "| epoch   6 |   400/ 1864 batches | lr 20.00 | ms/batch 43.07 | loss  5.61 | ppl   272.54\n",
            "| epoch   6 |   600/ 1864 batches | lr 20.00 | ms/batch 43.07 | loss  5.56 | ppl   259.01\n",
            "| epoch   6 |   800/ 1864 batches | lr 20.00 | ms/batch 43.07 | loss  5.59 | ppl   269.02\n",
            "| epoch   6 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.08 | loss  5.57 | ppl   262.83\n",
            "| epoch   6 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.21 | loss  5.57 | ppl   263.26\n",
            "| epoch   6 |  1400/ 1864 batches | lr 20.00 | ms/batch 43.18 | loss  5.51 | ppl   246.22\n",
            "| epoch   6 |  1600/ 1864 batches | lr 20.00 | ms/batch 43.32 | loss  5.48 | ppl   238.76\n",
            "| epoch   6 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.21 | loss  5.49 | ppl   242.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 84.23s | valid loss  5.39 | valid ppl   220.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1864 batches | lr 20.00 | ms/batch 43.49 | loss  5.49 | ppl   241.17\n",
            "| epoch   7 |   400/ 1864 batches | lr 20.00 | ms/batch 43.13 | loss  5.45 | ppl   231.82\n",
            "| epoch   7 |   600/ 1864 batches | lr 20.00 | ms/batch 43.19 | loss  5.41 | ppl   222.86\n",
            "| epoch   7 |   800/ 1864 batches | lr 20.00 | ms/batch 43.18 | loss  5.46 | ppl   234.29\n",
            "| epoch   7 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.17 | loss  5.44 | ppl   230.72\n",
            "| epoch   7 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.08 | loss  5.44 | ppl   231.01\n",
            "| epoch   7 |  1400/ 1864 batches | lr 20.00 | ms/batch 43.15 | loss  5.38 | ppl   217.05\n",
            "| epoch   7 |  1600/ 1864 batches | lr 20.00 | ms/batch 43.12 | loss  5.35 | ppl   211.38\n",
            "| epoch   7 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.09 | loss  5.37 | ppl   215.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 84.22s | valid loss  5.31 | valid ppl   203.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1864 batches | lr 20.00 | ms/batch 43.40 | loss  5.37 | ppl   215.86\n",
            "| epoch   8 |   400/ 1864 batches | lr 20.00 | ms/batch 43.20 | loss  5.33 | ppl   207.33\n",
            "| epoch   8 |   600/ 1864 batches | lr 20.00 | ms/batch 43.15 | loss  5.30 | ppl   201.18\n",
            "| epoch   8 |   800/ 1864 batches | lr 20.00 | ms/batch 43.18 | loss  5.35 | ppl   211.60\n",
            "| epoch   8 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.21 | loss  5.34 | ppl   209.47\n",
            "| epoch   8 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.26 | loss  5.35 | ppl   209.94\n",
            "| epoch   8 |  1400/ 1864 batches | lr 20.00 | ms/batch 43.20 | loss  5.28 | ppl   196.07\n",
            "| epoch   8 |  1600/ 1864 batches | lr 20.00 | ms/batch 43.30 | loss  5.26 | ppl   192.73\n",
            "| epoch   8 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.17 | loss  5.28 | ppl   197.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 84.32s | valid loss  5.28 | valid ppl   196.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1864 batches | lr 20.00 | ms/batch 43.33 | loss  5.29 | ppl   198.42\n",
            "| epoch   9 |   400/ 1864 batches | lr 20.00 | ms/batch 43.26 | loss  5.25 | ppl   190.82\n",
            "| epoch   9 |   600/ 1864 batches | lr 20.00 | ms/batch 43.34 | loss  5.23 | ppl   186.19\n",
            "| epoch   9 |   800/ 1864 batches | lr 20.00 | ms/batch 43.42 | loss  5.28 | ppl   196.28\n",
            "| epoch   9 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.33 | loss  5.27 | ppl   193.93\n",
            "| epoch   9 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.23 | loss  5.27 | ppl   194.54\n",
            "| epoch   9 |  1400/ 1864 batches | lr 20.00 | ms/batch 43.28 | loss  5.20 | ppl   181.16\n",
            "| epoch   9 |  1600/ 1864 batches | lr 20.00 | ms/batch 43.22 | loss  5.19 | ppl   179.35\n",
            "| epoch   9 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.26 | loss  5.21 | ppl   183.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 84.45s | valid loss  5.24 | valid ppl   188.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1864 batches | lr 20.00 | ms/batch 43.49 | loss  5.21 | ppl   183.74\n",
            "| epoch  10 |   400/ 1864 batches | lr 20.00 | ms/batch 43.29 | loss  5.18 | ppl   177.38\n",
            "| epoch  10 |   600/ 1864 batches | lr 20.00 | ms/batch 43.06 | loss  5.16 | ppl   173.91\n",
            "| epoch  10 |   800/ 1864 batches | lr 20.00 | ms/batch 43.24 | loss  5.21 | ppl   183.90\n",
            "| epoch  10 |  1000/ 1864 batches | lr 20.00 | ms/batch 43.27 | loss  5.21 | ppl   182.51\n",
            "| epoch  10 |  1200/ 1864 batches | lr 20.00 | ms/batch 43.29 | loss  5.21 | ppl   182.60\n",
            "| epoch  10 |  1400/ 1864 batches | lr 20.00 | ms/batch 43.29 | loss  5.14 | ppl   171.02\n",
            "| epoch  10 |  1600/ 1864 batches | lr 20.00 | ms/batch 43.25 | loss  5.13 | ppl   169.23\n",
            "| epoch  10 |  1800/ 1864 batches | lr 20.00 | ms/batch 43.28 | loss  5.15 | ppl   172.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 84.40s | valid loss  5.20 | valid ppl   181.50\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKoBi141Y_JM"
      },
      "source": [
        "### Dynamic Quantization\n",
        "\n",
        "We quantize the LSTM and Linear layers of our model using the PyTorch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyiM5evPf_g7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3c4e6f-b2fd-4885-bbb3-4896ca53818e"
      },
      "source": [
        "model.to(\"cpu\")\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(33278, 512)\n",
            "  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)\n",
            "  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5e4zifiZMRr"
      },
      "source": [
        "### Memory and Performance Comparison "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4IJjispf_g7"
      },
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSPdNkP4Iljy",
        "outputId": "0fae0807-c558-4b50-d910-fecd41447395"
      },
      "source": [
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size (MB): 113.945551\n",
            "Size (MB): 79.73967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unrg9UGdf_g7"
      },
      "source": [
        "torch.set_num_threads(1)\n",
        "def time_model_evaluation(model, test_data):\n",
        "    s = time.time()\n",
        "    loss = evaluate(model, test_data)\n",
        "    elapsed = time.time() - s\n",
        "    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gytz2mqAH7sz",
        "outputId": "7a75eac1-0b18-45db-ab1b-2885390b0bdd"
      },
      "source": [
        "time_model_evaluation(model, test_data)\n",
        "time_model_evaluation(quantized_model, test_data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 5.152\n",
            "elapsed time (seconds): 112.1\n",
            "loss: 5.151\n",
            "elapsed time (seconds): 83.7\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}